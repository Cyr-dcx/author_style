{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b731f8af",
   "metadata": {},
   "source": [
    "## utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477c51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "def csv_to_dataframes(output='ps'):\n",
    "    ''' Returns 2 dataframes\n",
    "\n",
    "    Extracts 1 dataframe with paragraphs and 1 dataframe with\n",
    "    sentences from a csv file. The csv files names' are parsed\n",
    "    assuming the following syntax:\n",
    "    \"author_name - title - publication_date.csv\"\n",
    "    '''\n",
    "    ##########################b#####################\n",
    "    ###y####  convert csv to df_paragraphs  ########\n",
    "    ################################################\n",
    "\n",
    "    # Get csv path ; the csv files are arrays of pre-selected* paragraphs\n",
    "    # that were extracted from raw txt files by * (cf. Lilou)\n",
    "    csv_path= \"/Users/cyrielle/code/Cyr-dcx/author_style/author_style/data/comp_aut\"\n",
    "\n",
    "\n",
    "    # Create a list of book names\n",
    "    books = [\n",
    "        csv_file for csv_file in os.listdir(csv_path)\n",
    "        if csv_file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Parsing csv file names to get author names, book titles and publishing date\n",
    "    # and putting these elements in lists that have the same index as the list 'books'\n",
    "    authors = [csv_file.split(' ')[0]+' '+csv_file.split(' ')[1] for csv_file in books]\n",
    "    titles = [csv_file.split(' - ')[1] for csv_file in books]\n",
    "    book_dates = [csv_file.split(' - ')[2].replace('.csv','') for csv_file in books]\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each book (in the list 'books'),\n",
    "    ## 1. create a dataframe with 1 paragraph per row\n",
    "    ## 2. create columns with fixed values for other features than text\n",
    "    ## 3. append the dataframe in the list 'dfs' of dataframes\n",
    "    ## containing the paragraphs from all books\n",
    "\n",
    "    for book in books:\n",
    "        ## 1.\n",
    "        df_temp = pd.read_csv(os.path.join(csv_path,book), header=None)\n",
    "        ## 2.\n",
    "        df_temp['author'] = authors[books.index(book)]\n",
    "        df_temp['title'] = titles[books.index(book)]\n",
    "        df_temp['book_date'] = book_dates[books.index(book)]\n",
    "        ## 3.\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    ## Concatenate all dataframes in 'dfs' to get\n",
    "    ## a single dataframe with paragraphs from all books\n",
    "    df_paragraphs = pd.concat([df for df in dfs], ignore_index = True, axis=0)\n",
    "    df_paragraphs.rename(mapper={0:\"text\"}, axis=1, inplace=True) # NB: The column name for the root_path text is explicitly called in a preprocessing function, it must be 'text'\n",
    "\n",
    "    ###############y########################################\n",
    "    ########  convert df_paragraphs to df_sentences  #######\n",
    "    #######################################b################\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each paragraph of our dataset (i.e. for each row in df_paragraph):\n",
    "    for i in range(df_paragraphs['text'].count()):\n",
    "\n",
    "        # Separate sentences with '. ' as a delimiter\n",
    "        # (careful: \"J. C.\", \"Mr.\", [...]) ignore ?\n",
    "        sentences = str(df_paragraphs.text[i]).split(\". \")\n",
    "\n",
    "        # Prepare columns with fixed values for Author_name, Title and Book_date,\n",
    "        # to assign each sentence of a paragraph to the same Author_name, Title and Book_date.\n",
    "        author_temp = [df_paragraphs.author[i] for k in range(len(sentences))]\n",
    "        title_temp = [df_paragraphs.title[i] for k in range(len(sentences))]\n",
    "        date_temp = [df_paragraphs.book_date[i] for k in range(len(sentences))]\n",
    "\n",
    "        # Concatenate the 4 previous lists to build a single dataframe\n",
    "        # containing all sentences of the i-th paragraph of df_paragraphs\n",
    "        data = [sentences, author_temp, title_temp, date_temp]\n",
    "        df_temp = pd.DataFrame(data).T\n",
    "\n",
    "        # Build the list of dataframes containing all sentences of our dataset\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    # Assemble the dataframe containing all sentences of our dataset\n",
    "    df_sentences = pd.concat(dfs, ignore_index = True, axis=0)\n",
    "    df_sentences.rename(mapper={0:\"text\", 1: 'author', 2:'title', 3 : 'book_date'}, axis=1, inplace=True)\n",
    "\n",
    "    if output == 'p':\n",
    "        return df_paragraphs\n",
    "    if output == 's':\n",
    "        return df_sentences\n",
    "    if output == 'ps':\n",
    "        return df_paragraphs, df_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c396a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = csv_to_dataframes(output=\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8addcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>book_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pendant que Nous franchissions la porte du Nor...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>« En 486 après Jésus-Christ, les troupes de Sy...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Van Eyck présenta La Vierge au chanoine Van de...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un an après l’insolence du soldat, Clovis rass...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les hommes se font une idée grotesque du temps...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18773</th>\n",
       "      <td>C’est à bord d’un train de la Southern Pacific...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18774</th>\n",
       "      <td>Quelle que soit, pour signer, la solution adop...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18775</th>\n",
       "      <td>Reste la possibilité d’aller faire un tour dan...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18776</th>\n",
       "      <td>Le lendemain matin, il se lève tard, traînant ...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18777</th>\n",
       "      <td>Il part en direction de la gare maritime du Ha...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18778 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        author  \\\n",
       "0      Pendant que Nous franchissions la porte du Nor...     GUTH Paul   \n",
       "1      « En 486 après Jésus-Christ, les troupes de Sy...     GUTH Paul   \n",
       "2      Van Eyck présenta La Vierge au chanoine Van de...     GUTH Paul   \n",
       "3      Un an après l’insolence du soldat, Clovis rass...     GUTH Paul   \n",
       "4      Les hommes se font une idée grotesque du temps...     GUTH Paul   \n",
       "...                                                  ...           ...   \n",
       "18773  C’est à bord d’un train de la Southern Pacific...  ECHENOZ Jean   \n",
       "18774  Quelle que soit, pour signer, la solution adop...  ECHENOZ Jean   \n",
       "18775  Reste la possibilité d’aller faire un tour dan...  ECHENOZ Jean   \n",
       "18776  Le lendemain matin, il se lève tard, traînant ...  ECHENOZ Jean   \n",
       "18777  Il part en direction de la gare maritime du Ha...  ECHENOZ Jean   \n",
       "\n",
       "                         title book_date  \n",
       "0      Si j_étais le Bon Dieu      1987  \n",
       "1      Si j_étais le Bon Dieu      1987  \n",
       "2      Si j_étais le Bon Dieu      1987  \n",
       "3      Si j_étais le Bon Dieu      1987  \n",
       "4      Si j_étais le Bon Dieu      1987  \n",
       "...                        ...       ...  \n",
       "18773                    Ravel      2006  \n",
       "18774                    Ravel      2006  \n",
       "18775                    Ravel      2006  \n",
       "18776                    Ravel      2006  \n",
       "18777                    Ravel      2006  \n",
       "\n",
       "[18778 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f5c79c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GARY Romain               0.079614\n",
       "GUTH Paul                 0.074289\n",
       "SARRAUTE Nathalie         0.073437\n",
       "MODIANO Patrick           0.073011\n",
       "JOFFO Joseph              0.063905\n",
       "VIAN Boris                0.047928\n",
       "SCHOENDOERFFER Pierre     0.047928\n",
       "QUEFFELEC Yann            0.047928\n",
       "DEBEAUVOIR Simone         0.047928\n",
       "GIONO Jean                0.047715\n",
       "CELINE Louis-Ferdinand    0.046171\n",
       "QUEFFELEC Henri           0.045745\n",
       "ECHENOZ Jean              0.044680\n",
       "DURAS Marguerite          0.037224\n",
       "CAMUS Albert              0.031153\n",
       "COHEN Albert              0.030834\n",
       "PEREC Georges             0.027532\n",
       "GASCAR Pierre             0.019438\n",
       "BAZIN Hervé              0.015976\n",
       "BASTIDE Francois-Regis    0.015976\n",
       "KESSEL Joseph             0.015976\n",
       "BAZIN Herve               0.015976\n",
       "DEBEAUVOIR SIMONE         0.015976\n",
       "GRAINVILLE Patrick        0.015976\n",
       "YOURCENAR Marguerite      0.015284\n",
       "SAGAN Fraçoise           0.002396\n",
       "Name: author, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author.value_counts()/pd.DataFrame(y).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4182b02",
   "metadata": {},
   "source": [
    "## preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b21ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import unidecode\n",
    "#import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "def preprocess(text,\n",
    "               punctuation=False,\n",
    "               lower_case=True,\n",
    "               remove_stopwords=False,\n",
    "               accents=True,\n",
    "               numbers=True,\n",
    "               lemmatize=False,\n",
    "               language='french'):\n",
    "\n",
    "    if numbers:\n",
    "        text = ''.join(char for char in text if not char.isdigit())\n",
    "    if punctuation:\n",
    "        text = ''.join(char for char in text if not char in string.punctuation)\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    if accents:\n",
    "        text = unidecode.unidecode(text)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(language))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = ' '.join(char for char in word_tokens if not char in stop_words)\n",
    "    if lemmatize:\n",
    "        text = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [lemmatizer.lemmatize(char) for char in text]\n",
    "        text = ' '.join(lemmatized)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_cleaned_column(df):\n",
    "    df[\"preprocess_data\"] = df['text'].apply(lambda x: preprocess(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"def return_token(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le texte de chaque token\n",
    "    return [X.text for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"def return_word_embedding(sentence):\n",
    "    # Vectoriser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le vecteur lié à chaque token\n",
    "    return [(X.vector) for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "def stopword_count(text):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopword_count = len([w for w in word_tokens if w in stop_words])\n",
    "    return stopword_count\n",
    "\n",
    "\n",
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    if total_length > 0:\n",
    "        return unique_word_length / total_length\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sentence_count(x):\n",
    "    if len(x.split()) >0:\n",
    "        return x.count('.') / len(x.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def features(df, output='p'):\n",
    "    if output=='p':\n",
    "\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['sentences_ratio'] = df['text'].apply(lambda x: 0 if len(x.split())==0 else x.count('.') / len(x.split()))\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n",
    "\n",
    "    elif output=='s':\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word_ratio'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5dcd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features(df, output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4ff85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>book_date</th>\n",
       "      <th>preprocess_data</th>\n",
       "      <th>word_ratio</th>\n",
       "      <th>unique_word</th>\n",
       "      <th>sentences_ratio</th>\n",
       "      <th>stopwords_ratio</th>\n",
       "      <th>vocab richness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pendant que Nous franchissions la porte du Nor...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>pendant que nous franchissions la porte du nor...</td>\n",
       "      <td>235</td>\n",
       "      <td>0.672340</td>\n",
       "      <td>0.051064</td>\n",
       "      <td>0.463830</td>\n",
       "      <td>0.533557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>« En 486 après Jésus-Christ, les troupes de Sy...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>&lt;&lt; en  apres jesus-christ, les troupes de syag...</td>\n",
       "      <td>261</td>\n",
       "      <td>0.685824</td>\n",
       "      <td>0.030651</td>\n",
       "      <td>0.467433</td>\n",
       "      <td>0.546584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Van Eyck présenta La Vierge au chanoine Van de...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>van eyck presenta la vierge au chanoine van de...</td>\n",
       "      <td>244</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>0.537975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un an après l’insolence du soldat, Clovis rass...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>un an apres l'insolence du soldat, clovis rass...</td>\n",
       "      <td>218</td>\n",
       "      <td>0.848624</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.658451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les hommes se font une idée grotesque du temps...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>les hommes se font une idee grotesque du temps...</td>\n",
       "      <td>214</td>\n",
       "      <td>0.649533</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>0.453271</td>\n",
       "      <td>0.533835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18773</th>\n",
       "      <td>C’est à bord d’un train de la Southern Pacific...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>c'est a bord d'un train de la southern pacific...</td>\n",
       "      <td>70</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.703297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18774</th>\n",
       "      <td>Quelle que soit, pour signer, la solution adop...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>quelle que soit, pour signer, la solution adop...</td>\n",
       "      <td>63</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.780822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18775</th>\n",
       "      <td>Reste la possibilité d’aller faire un tour dan...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>reste la possibilite d'aller faire un tour dan...</td>\n",
       "      <td>64</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.619565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18776</th>\n",
       "      <td>Le lendemain matin, il se lève tard, traînant ...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>le lendemain matin, il se leve tard, trainant ...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18777</th>\n",
       "      <td>Il part en direction de la gare maritime du Ha...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>il part en direction de la gare maritime du ha...</td>\n",
       "      <td>38</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18778 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        author  \\\n",
       "0      Pendant que Nous franchissions la porte du Nor...     GUTH Paul   \n",
       "1      « En 486 après Jésus-Christ, les troupes de Sy...     GUTH Paul   \n",
       "2      Van Eyck présenta La Vierge au chanoine Van de...     GUTH Paul   \n",
       "3      Un an après l’insolence du soldat, Clovis rass...     GUTH Paul   \n",
       "4      Les hommes se font une idée grotesque du temps...     GUTH Paul   \n",
       "...                                                  ...           ...   \n",
       "18773  C’est à bord d’un train de la Southern Pacific...  ECHENOZ Jean   \n",
       "18774  Quelle que soit, pour signer, la solution adop...  ECHENOZ Jean   \n",
       "18775  Reste la possibilité d’aller faire un tour dan...  ECHENOZ Jean   \n",
       "18776  Le lendemain matin, il se lève tard, traînant ...  ECHENOZ Jean   \n",
       "18777  Il part en direction de la gare maritime du Ha...  ECHENOZ Jean   \n",
       "\n",
       "                         title book_date  \\\n",
       "0      Si j_étais le Bon Dieu      1987   \n",
       "1      Si j_étais le Bon Dieu      1987   \n",
       "2      Si j_étais le Bon Dieu      1987   \n",
       "3      Si j_étais le Bon Dieu      1987   \n",
       "4      Si j_étais le Bon Dieu      1987   \n",
       "...                        ...       ...   \n",
       "18773                    Ravel      2006   \n",
       "18774                    Ravel      2006   \n",
       "18775                    Ravel      2006   \n",
       "18776                    Ravel      2006   \n",
       "18777                    Ravel      2006   \n",
       "\n",
       "                                         preprocess_data  word_ratio  \\\n",
       "0      pendant que nous franchissions la porte du nor...         235   \n",
       "1      << en  apres jesus-christ, les troupes de syag...         261   \n",
       "2      van eyck presenta la vierge au chanoine van de...         244   \n",
       "3      un an apres l'insolence du soldat, clovis rass...         218   \n",
       "4      les hommes se font une idee grotesque du temps...         214   \n",
       "...                                                  ...         ...   \n",
       "18773  c'est a bord d'un train de la southern pacific...          70   \n",
       "18774  quelle que soit, pour signer, la solution adop...          63   \n",
       "18775  reste la possibilite d'aller faire un tour dan...          64   \n",
       "18776  le lendemain matin, il se leve tard, trainant ...          60   \n",
       "18777  il part en direction de la gare maritime du ha...          38   \n",
       "\n",
       "       unique_word  sentences_ratio  stopwords_ratio  vocab richness  \n",
       "0         0.672340         0.051064         0.463830        0.533557  \n",
       "1         0.685824         0.030651         0.467433        0.546584  \n",
       "2         0.688525         0.049180         0.422131        0.537975  \n",
       "3         0.848624         0.045872         0.284404        0.658451  \n",
       "4         0.649533         0.051402         0.453271        0.533835  \n",
       "...            ...              ...              ...             ...  \n",
       "18773     0.871429         0.028571         0.428571        0.703297  \n",
       "18774     0.857143         0.031746         0.412698        0.780822  \n",
       "18775     0.828125         0.046875         0.562500        0.619565  \n",
       "18776     0.866667         0.016667         0.383333        0.785714  \n",
       "18777     0.842105         0.078947         0.473684        0.760000  \n",
       "\n",
       "[18778 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b9f2c",
   "metadata": {},
   "source": [
    "## pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6727f201",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t8/clhcfp2n5sgcs4b81syf8xym0000gp/T/ipykernel_21578/238798827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv_to_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t8/clhcfp2n5sgcs4b81syf8xym0000gp/T/ipykernel_21578/1410627106.py\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(df, output)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences_ratio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n\u001b[0m\u001b[1;32m     97\u001b[0m     )) == 0 else (stopword_count(x) / len(x.split())))\n\u001b[1;32m     98\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab richness'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_richness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/author_style/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/author_style/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/author_style/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1100\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/author_style/lib/python3.8/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/t8/clhcfp2n5sgcs4b81syf8xym0000gp/T/ipykernel_21578/1410627106.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences_ratio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n\u001b[0;32m---> 97\u001b[0;31m     )) == 0 else (stopword_count(x) / len(x.split())))\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab richness'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_richness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t8/clhcfp2n5sgcs4b81syf8xym0000gp/T/ipykernel_21578/1410627106.py\u001b[0m in \u001b[0;36mstopword_count\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstopword_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mstopword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstopword_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/author_style/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     ]\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/author_style/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     return [\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     ]\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/author_style/lib/python3.8/site-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = csv_to_dataframes(output='p')\n",
    "df = features(df, output='p')\n",
    "\n",
    "#selection de X et y dans le dataframe df\n",
    "X = df[['preprocess_data','unique_word',\n",
    "        'word_ratio','sentences_ratio',\n",
    "        'stopwords_ratio','vocab richness']]\n",
    "y = df[\"author\"]\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_transformer = LabelEncoder()\n",
    "y = cat_transformer.fit_transform(y)\n",
    "\n",
    "# transform X features\n",
    "column_trans = ColumnTransformer(\n",
    "    [('vec', TfidfVectorizer(), 'preprocess_data')], remainder='passthrough')\n",
    "\n",
    "X_combined = column_trans.fit_transform(X[[\n",
    "    'preprocess_data','unique_word',\n",
    "    'word_ratio','sentences_ratio',\n",
    "    'stopwords_ratio','vocab richness'\n",
    "]])\n",
    "\n",
    "#split date\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.30, random_state=42)\n",
    "\n",
    "#model\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "model_trained = nb_model()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "print(model_trained.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "997242cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_result = cross_val_score(MultinomialNB(),X_combined,y, cv=10, groups=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4be570fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08233012936715567"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b03b7",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f67a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dataframes_ajar(output='ps'):\n",
    "    ''' Returns 2 dataframes\n",
    "\n",
    "    Extracts 1 dataframe with paragraphs and 1 dataframe with\n",
    "    sentences from a csv file. The csv files names' are parsed\n",
    "    assuming the following syntax:\n",
    "    \"author_name - title - publication_date.csv\"\n",
    "    '''\n",
    "    ################################################\n",
    "    ########  convert csv to df_paragraphs  ########\n",
    "    ################################################\n",
    "\n",
    "    # Get csv path ; the csv files are arrays of pre-selected* paragraphs\n",
    "    # that were extracted from raw txt files by * (cf. Lilou)\n",
    "    csv_path= \"/Users/cyrielle/code/Cyr-dcx/author_style/author_style/data/txt_ajar/\"\n",
    "\n",
    "\n",
    "    # Create a list of book names\n",
    "    books = [\n",
    "        csv_file for csv_file in os.listdir(csv_path)\n",
    "        if csv_file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Parsing csv file names to get author names, book titles and publishing date\n",
    "    # and putting these elements in lists that have the same index as the list 'books'\n",
    "    authors = [csv_file.split(' ')[0]+' '+csv_file.split(' ')[1] for csv_file in books]\n",
    "    titles = [csv_file.split(' - ')[1] for csv_file in books]\n",
    "    book_dates = [csv_file.split(' - ')[2].replace('.csv','') for csv_file in books]\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each book (in the list 'books'),\n",
    "    ## 1. create a dataframe with 1 paragraph per row\n",
    "    ## 2. create columns with fixed values for other features than text\n",
    "    ## 3. append the dataframe in the list 'dfs' of dataframes\n",
    "    ## containing the paragraphs from all books\n",
    "\n",
    "    for book in books:\n",
    "        ## 1.\n",
    "        df_temp = pd.read_csv(os.path.join(csv_path,book), header=None)\n",
    "        ## 2.\n",
    "        df_temp['author'] = authors[books.index(book)]\n",
    "        df_temp['title'] = titles[books.index(book)]\n",
    "        df_temp['book_date'] = book_dates[books.index(book)]\n",
    "        ## 3.\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    ## Concatenate all dataframes in 'dfs' to get\n",
    "    ## a single dataframe with paragraphs from all books\n",
    "    df_paragraphs = pd.concat([df for df in dfs], ignore_index = True, axis=0)\n",
    "    df_paragraphs.rename(mapper={0:\"text\"}, axis=1, inplace=True) # NB: The column name for the root_path text is explicitly called in a preprocessing function, it must be 'text'\n",
    "\n",
    "    ########################################################\n",
    "    ########  convert df_paragraphs to df_sentences  #######\n",
    "    #######################################b################\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each paragraph of our dataset (i.e. for each row in df_paragraph):\n",
    "    for i in range(df_paragraphs['text'].count()):\n",
    "\n",
    "        # Separate sentences with '. ' as a delimiter\n",
    "        # (careful: \"J. C.\", \"Mr.\", [...]) ignore ?\n",
    "        sentences = str(df_paragraphs.text[i]).split(\". \")\n",
    "\n",
    "        # Prepare columns with fixed values for Author_name, Title and Book_date,\n",
    "        # to assign each sentence of a paragraph to the same Author_name, Title and Book_date.\n",
    "        author_temp = [df_paragraphs.author[i] for k in range(len(sentences))]\n",
    "        title_temp = [df_paragraphs.title[i] for k in range(len(sentences))]\n",
    "        date_temp = [df_paragraphs.book_date[i] for k in range(len(sentences))]\n",
    "\n",
    "        # Concatenate the 4 previous lists to build a single dataframe\n",
    "        # containing all sentences of the i-th paragraph of df_paragraphs\n",
    "        data = [sentences, author_temp, title_temp, date_temp]\n",
    "        df_temp = pd.DataFrame(data).T\n",
    "\n",
    "        # Build the list of dataframes containing all sentences of our dataset\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    # Assemble the dataframe containing all sentences of our dataset\n",
    "    df_sentences = pd.concat(dfs, ignore_index = True, axis=0)\n",
    "    df_sentences.rename(mapper={0:\"text\", 1: 'author', 2:'title', 3 : 'book_date'}, axis=1, inplace=True)\n",
    "\n",
    "    if output == 'p':\n",
    "        return df_paragraphs\n",
    "    if output == 's':\n",
    "        return df_sentences\n",
    "    if output == 'ps':\n",
    "        return df_paragraphs, df_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d61b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ajar = csv_to_dataframes_ajar(output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4da24f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>book_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le drame a éclaté le surlendemain, lorsque je ...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>Gros-Calin</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On peut imaginer, bref, dans quel état je fus ...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>Gros-Calin</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Je savais qu’il aimait beaucoup l’eau et je ne...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>Gros-Calin</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je me rappelai également que je venais justeme...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>Gros-Calin</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J’ai donc dit que Gros-Câlin est très beau. Lo...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>Gros-Calin</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>- Je vous jure que je vous dis la vérité, Mada...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>La vie devant soi</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>- Tiens, qu'est-ce qu'il a? a demandé Madame R...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>La vie devant soi</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>- Pas maintenant, Madame Rosa. On vous a pas d...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>La vie devant soi</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>- Ah, voilà notre petit Momo qui vient aux nou...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>La vie devant soi</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Je suis allé dire à Madame Rosa qu'il y avait ...</td>\n",
       "      <td>AJAR Emile</td>\n",
       "      <td>La vie devant soi</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text      author  \\\n",
       "0    Le drame a éclaté le surlendemain, lorsque je ...  AJAR Emile   \n",
       "1    On peut imaginer, bref, dans quel état je fus ...  AJAR Emile   \n",
       "2    Je savais qu’il aimait beaucoup l’eau et je ne...  AJAR Emile   \n",
       "3    Je me rappelai également que je venais justeme...  AJAR Emile   \n",
       "4    J’ai donc dit que Gros-Câlin est très beau. Lo...  AJAR Emile   \n",
       "..                                                 ...         ...   \n",
       "895  - Je vous jure que je vous dis la vérité, Mada...  AJAR Emile   \n",
       "896  - Tiens, qu'est-ce qu'il a? a demandé Madame R...  AJAR Emile   \n",
       "897  - Pas maintenant, Madame Rosa. On vous a pas d...  AJAR Emile   \n",
       "898  - Ah, voilà notre petit Momo qui vient aux nou...  AJAR Emile   \n",
       "899  Je suis allé dire à Madame Rosa qu'il y avait ...  AJAR Emile   \n",
       "\n",
       "                 title book_date  \n",
       "0           Gros-Calin      1975  \n",
       "1           Gros-Calin      1975  \n",
       "2           Gros-Calin      1975  \n",
       "3           Gros-Calin      1975  \n",
       "4           Gros-Calin      1975  \n",
       "..                 ...       ...  \n",
       "895  La vie devant soi      1975  \n",
       "896  La vie devant soi      1975  \n",
       "897  La vie devant soi      1975  \n",
       "898  La vie devant soi      1975  \n",
       "899  La vie devant soi      1975  \n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce4e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ajar = features(data_ajar, output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a6414e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['preprocess_data','unique_word',\n",
    "        'word_ratio','sentences_ratio',\n",
    "        'stopwords_ratio','vocab richness']]\n",
    "\n",
    "# transform X features\n",
    "column_trans = ColumnTransformer(\n",
    "    [('vec', TfidfVectorizer(), 'preprocess_data')], remainder='passthrough')\n",
    "\n",
    "X_combined = column_trans.fit_transform(X[[\n",
    "    'preprocess_data','unique_word',\n",
    "    'word_ratio','sentences_ratio',\n",
    "    'stopwords_ratio','vocab richness'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0576c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = pd.Series(nb_model.predict(X_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a461d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    18751\n",
       "22       27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99867920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GARY Romain'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_transformer.inverse_transform(np.array((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa7db9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SARRAUTE Nathalie'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_transformer.inverse_transform(np.array((22,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b47128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
