{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99450d4c",
   "metadata": {},
   "source": [
    "## utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c1e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "def csv_to_dataframes(output='ps'):\n",
    "    ''' Returns 2 dataframes\n",
    "\n",
    "    Extracts 1 dataframe with paragraphs and 1 dataframe with\n",
    "    sentences from a csv file. The csv files names' are parsed\n",
    "    assuming the following syntax:\n",
    "    \"author_name - title - publication_date.csv\"\n",
    "    '''\n",
    "    ##########################b#####################\n",
    "    ###y####  convert csv to df_paragraphs  ########\n",
    "    ################################################\n",
    "\n",
    "    # Get csv path ; the csv files are arrays of pre-selected* paragraphs\n",
    "    # that were extracted from raw txt files by * (cf. Lilou)\n",
    "    csv_path= \"/Users/cyrielle/code/Cyr-dcx/author_style/author_style/data/comp_aut\"\n",
    "\n",
    "\n",
    "    # Create a list of book names\n",
    "    books = [\n",
    "        csv_file for csv_file in os.listdir(csv_path)\n",
    "        if csv_file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Parsing csv file names to get author names, book titles and publishing date\n",
    "    # and putting these elements in lists that have the same index as the list 'books'\n",
    "    authors = [csv_file.split(' ')[0]+' '+csv_file.split(' ')[1] for csv_file in books]\n",
    "    titles = [csv_file.split(' - ')[1] for csv_file in books]\n",
    "    book_dates = [csv_file.split(' - ')[2].replace('.csv','') for csv_file in books]\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each book (in the list 'books'),\n",
    "    ## 1. create a dataframe with 1 paragraph per row\n",
    "    ## 2. create columns with fixed values for other features than text\n",
    "    ## 3. append the dataframe in the list 'dfs' of dataframes\n",
    "    ## containing the paragraphs from all books\n",
    "\n",
    "    for book in books:\n",
    "        ## 1.\n",
    "        df_temp = pd.read_csv(os.path.join(csv_path,book), header=None)\n",
    "        ## 2.\n",
    "        df_temp['author'] = authors[books.index(book)]\n",
    "        df_temp['title'] = titles[books.index(book)]\n",
    "        df_temp['book_date'] = book_dates[books.index(book)]\n",
    "        ## 3.\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    ## Concatenate all dataframes in 'dfs' to get\n",
    "    ## a single dataframe with paragraphs from all books\n",
    "    df_paragraphs = pd.concat([df for df in dfs], ignore_index = True, axis=0)\n",
    "    df_paragraphs.rename(mapper={0:\"text\"}, axis=1, inplace=True) # NB: The column name for the root_path text is explicitly called in a preprocessing function, it must be 'text'\n",
    "\n",
    "    ###############y########################################\n",
    "    ########  convert df_paragraphs to df_sentences  #######\n",
    "    #######################################b################\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each paragraph of our dataset (i.e. for each row in df_paragraph):\n",
    "    for i in range(df_paragraphs['text'].count()):\n",
    "\n",
    "        # Separate sentences with '. ' as a delimiter\n",
    "        # (careful: \"J. C.\", \"Mr.\", [...]) ignore ?\n",
    "        sentences = str(df_paragraphs.text[i]).split(\". \")\n",
    "\n",
    "        # Prepare columns with fixed values for Author_name, Title and Book_date,\n",
    "        # to assign each sentence of a paragraph to the same Author_name, Title and Book_date.\n",
    "        author_temp = [df_paragraphs.author[i] for k in range(len(sentences))]\n",
    "        title_temp = [df_paragraphs.title[i] for k in range(len(sentences))]\n",
    "        date_temp = [df_paragraphs.book_date[i] for k in range(len(sentences))]\n",
    "\n",
    "        # Concatenate the 4 previous lists to build a single dataframe\n",
    "        # containing all sentences of the i-th paragraph of df_paragraphs\n",
    "        data = [sentences, author_temp, title_temp, date_temp]\n",
    "        df_temp = pd.DataFrame(data).T\n",
    "\n",
    "        # Build the list of dataframes containing all sentences of our dataset\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    # Assemble the dataframe containing all sentences of our dataset\n",
    "    df_sentences = pd.concat(dfs, ignore_index = True, axis=0)\n",
    "    df_sentences.rename(mapper={0:\"text\", 1: 'author', 2:'title', 3 : 'book_date'}, axis=1, inplace=True)\n",
    "\n",
    "    if output == 'p':\n",
    "        return df_paragraphs\n",
    "    if output == 's':\n",
    "        return df_sentences\n",
    "    if output == 'ps':\n",
    "        return df_paragraphs, df_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4649e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = csv_to_dataframes(output=\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60fb586c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>book_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pendant que Nous franchissions la porte du Nor...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>« En 486 après Jésus-Christ, les troupes de Sy...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Van Eyck présenta La Vierge au chanoine Van de...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un an après l’insolence du soldat, Clovis rass...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les hommes se font une idée grotesque du temps...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17774</th>\n",
       "      <td>C’est à bord d’un train de la Southern Pacific...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17775</th>\n",
       "      <td>Quelle que soit, pour signer, la solution adop...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17776</th>\n",
       "      <td>Reste la possibilité d’aller faire un tour dan...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17777</th>\n",
       "      <td>Le lendemain matin, il se lève tard, traînant ...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17778</th>\n",
       "      <td>Il part en direction de la gare maritime du Ha...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17779 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        author  \\\n",
       "0      Pendant que Nous franchissions la porte du Nor...     GUTH Paul   \n",
       "1      « En 486 après Jésus-Christ, les troupes de Sy...     GUTH Paul   \n",
       "2      Van Eyck présenta La Vierge au chanoine Van de...     GUTH Paul   \n",
       "3      Un an après l’insolence du soldat, Clovis rass...     GUTH Paul   \n",
       "4      Les hommes se font une idée grotesque du temps...     GUTH Paul   \n",
       "...                                                  ...           ...   \n",
       "17774  C’est à bord d’un train de la Southern Pacific...  ECHENOZ Jean   \n",
       "17775  Quelle que soit, pour signer, la solution adop...  ECHENOZ Jean   \n",
       "17776  Reste la possibilité d’aller faire un tour dan...  ECHENOZ Jean   \n",
       "17777  Le lendemain matin, il se lève tard, traînant ...  ECHENOZ Jean   \n",
       "17778  Il part en direction de la gare maritime du Ha...  ECHENOZ Jean   \n",
       "\n",
       "                         title book_date  \n",
       "0      Si j_étais le Bon Dieu      1987  \n",
       "1      Si j_étais le Bon Dieu      1987  \n",
       "2      Si j_étais le Bon Dieu      1987  \n",
       "3      Si j_étais le Bon Dieu      1987  \n",
       "4      Si j_étais le Bon Dieu      1987  \n",
       "...                        ...       ...  \n",
       "17774                    Ravel      2006  \n",
       "17775                    Ravel      2006  \n",
       "17776                    Ravel      2006  \n",
       "17777                    Ravel      2006  \n",
       "17778                    Ravel      2006  \n",
       "\n",
       "[17779 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3182e5",
   "metadata": {},
   "source": [
    "## preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f606e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import unidecode\n",
    "#import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "def preprocess(text,\n",
    "               punctuation=False,\n",
    "               lower_case=True,\n",
    "               remove_stopwords=False,\n",
    "               accents=True,\n",
    "               numbers=True,\n",
    "               lemmatize=False,\n",
    "               language='french'):\n",
    "\n",
    "    if numbers:\n",
    "        text = ''.join(char for char in text if not char.isdigit())\n",
    "    if punctuation:\n",
    "        text = ''.join(char for char in text if not char in string.punctuation)\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    if accents:\n",
    "        text = unidecode.unidecode(text)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(language))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = ' '.join(char for char in word_tokens if not char in stop_words)\n",
    "    if lemmatize:\n",
    "        text = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [lemmatizer.lemmatize(char) for char in text]\n",
    "        text = ' '.join(lemmatized)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_cleaned_column(df):\n",
    "    df[\"preprocess_data\"] = df['text'].apply(lambda x: preprocess(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"def return_token(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le texte de chaque token\n",
    "    return [X.text for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"def return_word_embedding(sentence):\n",
    "    # Vectoriser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le vecteur lié à chaque token\n",
    "    return [(X.vector) for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "def stopword_count(text):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopword_count = len([w for w in word_tokens if w in stop_words])\n",
    "    return stopword_count\n",
    "\n",
    "\n",
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    if total_length > 0:\n",
    "        return unique_word_length / total_length\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sentence_count(x):\n",
    "    if len(x.split()) >0:\n",
    "        return x.count('.') / len(x.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def features(df, output='p'):\n",
    "    if output=='p':\n",
    "\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['sentences_ratio'] = df['text'].apply(lambda x: 0 if len(x.split())==0 else x.count('.') / len(x.split()))\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n",
    "\n",
    "    elif output=='s':\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word_ratio'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12537b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features(df, output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "390abff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>book_date</th>\n",
       "      <th>preprocess_data</th>\n",
       "      <th>word_ratio</th>\n",
       "      <th>unique_word</th>\n",
       "      <th>sentences_ratio</th>\n",
       "      <th>stopwords_ratio</th>\n",
       "      <th>vocab richness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pendant que Nous franchissions la porte du Nor...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>pendant que nous franchissions la porte du nor...</td>\n",
       "      <td>235</td>\n",
       "      <td>0.672340</td>\n",
       "      <td>0.051064</td>\n",
       "      <td>0.463830</td>\n",
       "      <td>0.533557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>« En 486 après Jésus-Christ, les troupes de Sy...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>&lt;&lt; en  apres jesus-christ, les troupes de syag...</td>\n",
       "      <td>261</td>\n",
       "      <td>0.685824</td>\n",
       "      <td>0.030651</td>\n",
       "      <td>0.467433</td>\n",
       "      <td>0.546584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Van Eyck présenta La Vierge au chanoine Van de...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>van eyck presenta la vierge au chanoine van de...</td>\n",
       "      <td>244</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>0.537975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Un an après l’insolence du soldat, Clovis rass...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>un an apres l'insolence du soldat, clovis rass...</td>\n",
       "      <td>218</td>\n",
       "      <td>0.848624</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.658451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les hommes se font une idée grotesque du temps...</td>\n",
       "      <td>GUTH Paul</td>\n",
       "      <td>Si j_étais le Bon Dieu</td>\n",
       "      <td>1987</td>\n",
       "      <td>les hommes se font une idee grotesque du temps...</td>\n",
       "      <td>214</td>\n",
       "      <td>0.649533</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>0.453271</td>\n",
       "      <td>0.533835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17774</th>\n",
       "      <td>C’est à bord d’un train de la Southern Pacific...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>c'est a bord d'un train de la southern pacific...</td>\n",
       "      <td>70</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.703297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17775</th>\n",
       "      <td>Quelle que soit, pour signer, la solution adop...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>quelle que soit, pour signer, la solution adop...</td>\n",
       "      <td>63</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.780822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17776</th>\n",
       "      <td>Reste la possibilité d’aller faire un tour dan...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>reste la possibilite d'aller faire un tour dan...</td>\n",
       "      <td>64</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.619565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17777</th>\n",
       "      <td>Le lendemain matin, il se lève tard, traînant ...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>le lendemain matin, il se leve tard, trainant ...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17778</th>\n",
       "      <td>Il part en direction de la gare maritime du Ha...</td>\n",
       "      <td>ECHENOZ Jean</td>\n",
       "      <td>Ravel</td>\n",
       "      <td>2006</td>\n",
       "      <td>il part en direction de la gare maritime du ha...</td>\n",
       "      <td>38</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17779 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        author  \\\n",
       "0      Pendant que Nous franchissions la porte du Nor...     GUTH Paul   \n",
       "1      « En 486 après Jésus-Christ, les troupes de Sy...     GUTH Paul   \n",
       "2      Van Eyck présenta La Vierge au chanoine Van de...     GUTH Paul   \n",
       "3      Un an après l’insolence du soldat, Clovis rass...     GUTH Paul   \n",
       "4      Les hommes se font une idée grotesque du temps...     GUTH Paul   \n",
       "...                                                  ...           ...   \n",
       "17774  C’est à bord d’un train de la Southern Pacific...  ECHENOZ Jean   \n",
       "17775  Quelle que soit, pour signer, la solution adop...  ECHENOZ Jean   \n",
       "17776  Reste la possibilité d’aller faire un tour dan...  ECHENOZ Jean   \n",
       "17777  Le lendemain matin, il se lève tard, traînant ...  ECHENOZ Jean   \n",
       "17778  Il part en direction de la gare maritime du Ha...  ECHENOZ Jean   \n",
       "\n",
       "                         title book_date  \\\n",
       "0      Si j_étais le Bon Dieu      1987   \n",
       "1      Si j_étais le Bon Dieu      1987   \n",
       "2      Si j_étais le Bon Dieu      1987   \n",
       "3      Si j_étais le Bon Dieu      1987   \n",
       "4      Si j_étais le Bon Dieu      1987   \n",
       "...                        ...       ...   \n",
       "17774                    Ravel      2006   \n",
       "17775                    Ravel      2006   \n",
       "17776                    Ravel      2006   \n",
       "17777                    Ravel      2006   \n",
       "17778                    Ravel      2006   \n",
       "\n",
       "                                         preprocess_data  word_ratio  \\\n",
       "0      pendant que nous franchissions la porte du nor...         235   \n",
       "1      << en  apres jesus-christ, les troupes de syag...         261   \n",
       "2      van eyck presenta la vierge au chanoine van de...         244   \n",
       "3      un an apres l'insolence du soldat, clovis rass...         218   \n",
       "4      les hommes se font une idee grotesque du temps...         214   \n",
       "...                                                  ...         ...   \n",
       "17774  c'est a bord d'un train de la southern pacific...          70   \n",
       "17775  quelle que soit, pour signer, la solution adop...          63   \n",
       "17776  reste la possibilite d'aller faire un tour dan...          64   \n",
       "17777  le lendemain matin, il se leve tard, trainant ...          60   \n",
       "17778  il part en direction de la gare maritime du ha...          38   \n",
       "\n",
       "       unique_word  sentences_ratio  stopwords_ratio  vocab richness  \n",
       "0         0.672340         0.051064         0.463830        0.533557  \n",
       "1         0.685824         0.030651         0.467433        0.546584  \n",
       "2         0.688525         0.049180         0.422131        0.537975  \n",
       "3         0.848624         0.045872         0.284404        0.658451  \n",
       "4         0.649533         0.051402         0.453271        0.533835  \n",
       "...            ...              ...              ...             ...  \n",
       "17774     0.871429         0.028571         0.428571        0.703297  \n",
       "17775     0.857143         0.031746         0.412698        0.780822  \n",
       "17776     0.828125         0.046875         0.562500        0.619565  \n",
       "17777     0.866667         0.016667         0.383333        0.785714  \n",
       "17778     0.842105         0.078947         0.473684        0.760000  \n",
       "\n",
       "[17779 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe6da7",
   "metadata": {},
   "source": [
    "## pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6529270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = csv_to_dataframes(output='p')\n",
    "df = features(df, output='p')\n",
    "\n",
    "#selection de X et y dans le dataframe df\n",
    "X = df[['preprocess_data','unique_word',\n",
    "        'word_ratio','sentences_ratio',\n",
    "        'stopwords_ratio','vocab richness']]\n",
    "y = df[\"author\"]\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_transformer = LabelEncoder()\n",
    "y = cat_transformer.fit_transform(y)\n",
    "\n",
    "\n",
    "#split date\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.30, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a4df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters to search\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1,1), (2,2)),\n",
    "    'nb__alpha': (0.1,1),}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bf460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_result = cross_val_score(MultinomialNB(),X_combined,y, cv=10, groups=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829e709",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f338ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dataframes_ajar(output='ps'):\n",
    "    ''' Returns 2 dataframes\n",
    "\n",
    "    Extracts 1 dataframe with paragraphs and 1 dataframe with\n",
    "    sentences from a csv file. The csv files names' are parsed\n",
    "    assuming the following syntax:\n",
    "    \"author_name - title - publication_date.csv\"\n",
    "    '''\n",
    "    ################################################\n",
    "    ########  convert csv to df_paragraphs  ########\n",
    "    ################################################\n",
    "\n",
    "    # Get csv path ; the csv files are arrays of pre-selected* paragraphs\n",
    "    # that were extracted from raw txt files by * (cf. Lilou)\n",
    "    csv_path= \"/Users/cyrielle/code/Cyr-dcx/author_style/author_style/data/txt_ajar/\"\n",
    "\n",
    "\n",
    "    # Create a list of book names\n",
    "    books = [\n",
    "        csv_file for csv_file in os.listdir(csv_path)\n",
    "        if csv_file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Parsing csv file names to get author names, book titles and publishing date\n",
    "    # and putting these elements in lists that have the same index as the list 'books'\n",
    "    authors = [csv_file.split(' ')[0]+' '+csv_file.split(' ')[1] for csv_file in books]\n",
    "    titles = [csv_file.split(' - ')[1] for csv_file in books]\n",
    "    book_dates = [csv_file.split(' - ')[2].replace('.csv','') for csv_file in books]\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each book (in the list 'books'),\n",
    "    ## 1. create a dataframe with 1 paragraph per row\n",
    "    ## 2. create columns with fixed values for other features than text\n",
    "    ## 3. append the dataframe in the list 'dfs' of dataframes\n",
    "    ## containing the paragraphs from all books\n",
    "\n",
    "    for book in books:\n",
    "        ## 1.\n",
    "        df_temp = pd.read_csv(os.path.join(csv_path,book), header=None)\n",
    "        ## 2.\n",
    "        df_temp['author'] = authors[books.index(book)]\n",
    "        df_temp['title'] = titles[books.index(book)]\n",
    "        df_temp['book_date'] = book_dates[books.index(book)]\n",
    "        ## 3.\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    ## Concatenate all dataframes in 'dfs' to get\n",
    "    ## a single dataframe with paragraphs from all books\n",
    "    df_paragraphs = pd.concat([df for df in dfs], ignore_index = True, axis=0)\n",
    "    df_paragraphs.rename(mapper={0:\"text\"}, axis=1, inplace=True) # NB: The column name for the root_path text is explicitly called in a preprocessing function, it must be 'text'\n",
    "\n",
    "    ########################################################\n",
    "    ########  convert df_paragraphs to df_sentences  #######\n",
    "    #######################################b################\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each paragraph of our dataset (i.e. for each row in df_paragraph):\n",
    "    for i in range(df_paragraphs['text'].count()):\n",
    "\n",
    "        # Separate sentences with '. ' as a delimiter\n",
    "        # (careful: \"J. C.\", \"Mr.\", [...]) ignore ?\n",
    "        sentences = str(df_paragraphs.text[i]).split(\". \")\n",
    "\n",
    "        # Prepare columns with fixed values for Author_name, Title and Book_date,\n",
    "        # to assign each sentence of a paragraph to the same Author_name, Title and Book_date.\n",
    "        author_temp = [df_paragraphs.author[i] for k in range(len(sentences))]\n",
    "        title_temp = [df_paragraphs.title[i] for k in range(len(sentences))]\n",
    "        date_temp = [df_paragraphs.book_date[i] for k in range(len(sentences))]\n",
    "\n",
    "        # Concatenate the 4 previous lists to build a single dataframe\n",
    "        # containing all sentences of the i-th paragraph of df_paragraphs\n",
    "        data = [sentences, author_temp, title_temp, date_temp]\n",
    "        df_temp = pd.DataFrame(data).T\n",
    "\n",
    "        # Build the list of dataframes containing all sentences of our dataset\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    # Assemble the dataframe containing all sentences of our dataset\n",
    "    df_sentences = pd.concat(dfs, ignore_index = True, axis=0)\n",
    "    df_sentences.rename(mapper={0:\"text\", 1: 'author', 2:'title', 3 : 'book_date'}, axis=1, inplace=True)\n",
    "\n",
    "    if output == 'p':\n",
    "        return df_paragraphs\n",
    "    if output == 's':\n",
    "        return df_sentences\n",
    "    if output == 'ps':\n",
    "        return df_paragraphs, df_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91daa84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ajar = csv_to_dataframes_ajar(output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ajar = features(data_ajar, output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['preprocess_data','unique_word',\n",
    "        'word_ratio','sentences_ratio',\n",
    "        'stopwords_ratio','vocab richness']]\n",
    "\n",
    "# transform X features\n",
    "column_trans = ColumnTransformer(\n",
    "    [('vec', TfidfVectorizer(), 'preprocess_data')], remainder='passthrough')\n",
    "\n",
    "X_combined = column_trans.fit_transform(X[[\n",
    "    'preprocess_data','unique_word',\n",
    "    'word_ratio','sentences_ratio',\n",
    "    'stopwords_ratio','vocab richness'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d319a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t8/clhcfp2n5sgcs4b81syf8xym0000gp/T/ipykernel_25857/2535015479.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "df.author.value_counts()/pd.DataFrame(y).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d23f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = pd.Series(nb_model.predict(X_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c811ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transformer.inverse_transform(np.array((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77235cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transformer.inverse_transform(np.array((22,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = (\"Ils s’arrachaient à leurs armoires à glace où ils étaient en train de scruter leurs visages. Se soulevaient sur leurs lits : « C’est servi, c’est servi », disait-elle. Elle rassemblait à table la famille, chacun caché dans son antre, solitaire, hargneux, épuisé. « Mais qu’ont-ils donc pour avoir l’air toujours vannés ? » disait-elle quand elle parlait à la cuisinière. Elle parlait à la cuisinière pendant des heures, s’agitant autour de la table, s’agitant toujours, préparant des potions pour eux ou des plats, elle parlait, critiquant les gens qui venaient à la maison, les amis : « et les cheveux d’une telle qui vont foncer, ils seront comme ceux de sa mère, et droits ; ils ont de la chance, ceux qui n’ont pas besoin de permanente ». – « Mademoiselle a de beaux cheveux », disait la cuisinière, « ils sont épais, ils sont beaux malgré qu’ils ne bouclent pas ». – « Et un tel, je suis sûre qu’il ne vous a pas laissé quelque chose. Ils sont avares, avares tous, et ils ont de l’argent, ils ont de l’argent, c’est dégoûtant. Et ils se privent de tout. Moi, je ne comprends pas ça. » – « Ah ! non, disait la cuisinière, non, ils ne l’emporteront pas avec eux. Et leur fille, elle n’est toujours pas mariée, et elle n’est pas mal, elle a de beaux cheveux, un petit nez, de jolis pieds aussi. » – « Oui, de beaux cheveux, c’est vrai, disait-elle, mais personne ne l’aime, vous savez, elle ne plaît pas. Ah ! C’est drôle vraiment ».Et il sentait filtrer de la cuisine la pensée humble et crasseuse, piétinante, piétinant toujours sur place, ...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
