{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8707b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "def csv_to_dataframes(output='ps'):\n",
    "    ''' Returns 2 dataframes\n",
    "\n",
    "    Extracts 1 dataframe with paragraphs and 1 dataframe with\n",
    "    sentences from a csv file. The csv files names' are parsed\n",
    "    assuming the following syntax:\n",
    "    \"author_name - title - publication_date.csv\"\n",
    "    '''\n",
    "    ##########################b#####################\n",
    "    ###y####  convert csv to df_paragraphs  ########\n",
    "    ################################################\n",
    "\n",
    "    # Get csv path ; the csv files are arrays of pre-selected* paragraphs\n",
    "    # that were extracted from raw txt files by * (cf. Lilou)\n",
    "    csv_path= \"/Users/cyrielle/code/Cyr-dcx/author_style/author_style/data/comp_aut\"\n",
    "\n",
    "\n",
    "    # Create a list of book names\n",
    "    books = [\n",
    "        csv_file for csv_file in os.listdir(csv_path)\n",
    "        if csv_file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Parsing csv file names to get author names, book titles and publishing date\n",
    "    # and putting these elements in lists that have the same index as the list 'books'\n",
    "    authors = [csv_file.split(' ')[0]+' '+csv_file.split(' ')[1] for csv_file in books]\n",
    "    titles = [csv_file.split(' - ')[1] for csv_file in books]\n",
    "    book_dates = [csv_file.split(' - ')[2].replace('.csv','') for csv_file in books]\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each book (in the list 'books'),\n",
    "    ## 1. create a dataframe with 1 paragraph per row\n",
    "    ## 2. create columns with fixed values for other features than text\n",
    "    ## 3. append the dataframe in the list 'dfs' of dataframes\n",
    "    ## containing the paragraphs from all books\n",
    "\n",
    "    for book in books:\n",
    "        ## 1.\n",
    "        df_temp = pd.read_csv(os.path.join(csv_path,book), header=None)\n",
    "        ## 2.\n",
    "        df_temp['author'] = authors[books.index(book)]\n",
    "        df_temp['title'] = titles[books.index(book)]\n",
    "        df_temp['book_date'] = book_dates[books.index(book)]\n",
    "        ## 3.\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    ## Concatenate all dataframes in 'dfs' to get\n",
    "    ## a single dataframe with paragraphs from all books\n",
    "    df_paragraphs = pd.concat([df for df in dfs], ignore_index = True, axis=0)\n",
    "    df_paragraphs.rename(mapper={0:\"text\"}, axis=1, inplace=True) # NB: The column name for the root_path text is explicitly called in a preprocessing function, it must be 'text'\n",
    "\n",
    "    ###############y########################################\n",
    "    ########  convert df_paragraphs to df_sentences  #######\n",
    "    #######################################b################\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each paragraph of our dataset (i.e. for each row in df_paragraph):\n",
    "    for i in range(df_paragraphs['text'].count()):\n",
    "\n",
    "        # Separate sentences with '. ' as a delimiter\n",
    "        # (careful: \"J. C.\", \"Mr.\", [...]) ignore ?\n",
    "        sentences = str(df_paragraphs.text[i]).split(\". \")\n",
    "\n",
    "        # Prepare columns with fixed values for Author_name, Title and Book_date,\n",
    "        # to assign each sentence of a paragraph to the same Author_name, Title and Book_date.\n",
    "        author_temp = [df_paragraphs.author[i] for k in range(len(sentences))]\n",
    "        title_temp = [df_paragraphs.title[i] for k in range(len(sentences))]\n",
    "        date_temp = [df_paragraphs.book_date[i] for k in range(len(sentences))]\n",
    "\n",
    "        # Concatenate the 4 previous lists to build a single dataframe\n",
    "        # containing all sentences of the i-th paragraph of df_paragraphs\n",
    "        data = [sentences, author_temp, title_temp, date_temp]\n",
    "        df_temp = pd.DataFrame(data).T\n",
    "\n",
    "        # Build the list of dataframes containing all sentences of our dataset\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    # Assemble the dataframe containing all sentences of our dataset\n",
    "    df_sentences = pd.concat(dfs, ignore_index = True, axis=0)\n",
    "    df_sentences.rename(mapper={0:\"text\", 1: 'author', 2:'title', 3 : 'book_date'}, axis=1, inplace=True)\n",
    "\n",
    "    if output == 'p':\n",
    "        return df_paragraphs\n",
    "    if output == 's':\n",
    "        return df_sentences\n",
    "    if output == 'ps':\n",
    "        return df_paragraphs, df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99eca329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = csv_to_dataframes(output=\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bba4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import unidecode\n",
    "#import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "def preprocess(text,\n",
    "               punctuation=False,\n",
    "               lower_case=True,\n",
    "               remove_stopwords=False,\n",
    "               accents=True,\n",
    "               numbers=True,\n",
    "               lemmatize=False,\n",
    "               language='french'):\n",
    "\n",
    "    if numbers:\n",
    "        text = ''.join(char for char in text if not char.isdigit())\n",
    "    if punctuation:\n",
    "        text = ''.join(char for char in text if not char in string.punctuation)\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    if accents:\n",
    "        text = unidecode.unidecode(text)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(language))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = ' '.join(char for char in word_tokens if not char in stop_words)\n",
    "    if lemmatize:\n",
    "        text = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [lemmatizer.lemmatize(char) for char in text]\n",
    "        text = ' '.join(lemmatized)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_cleaned_column(df):\n",
    "    df[\"preprocess_data\"] = df['text'].apply(lambda x: preprocess(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"def return_token(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le texte de chaque token\n",
    "    return [X.text for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"def return_word_embedding(sentence):\n",
    "    # Vectoriser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le vecteur lié à chaque token\n",
    "    return [(X.vector) for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "def stopword_count(text):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopword_count = len([w for w in word_tokens if w in stop_words])\n",
    "    return stopword_count\n",
    "\n",
    "\n",
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    if total_length > 0:\n",
    "        return unique_word_length / total_length\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sentence_count(x):\n",
    "    if len(x.split()) >0:\n",
    "        return x.count('.') / len(x.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def features(df, output='p'):\n",
    "    if output=='p':\n",
    "\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['sentences_ratio'] = df['text'].apply(lambda x: 0 if len(x.split())==0 else x.count('.') / len(x.split()))\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n",
    "\n",
    "    elif output=='s':\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word_ratio'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9457d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features(df, output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3898c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = csv_to_dataframes(output='p')\n",
    "df = features(df, output='p')\n",
    "\n",
    "#selection de X et y dans le dataframe df\n",
    "X = df[['preprocess_data','unique_word',\n",
    "        'word_ratio','sentences_ratio',\n",
    "        'stopwords_ratio','vocab richness']]\n",
    "y = df[\"author\"]\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_transformer = LabelEncoder()\n",
    "y = cat_transformer.fit_transform(y)\n",
    "\n",
    "\n",
    "#split date\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34c26afc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocess',\n",
       "   ColumnTransformer(remainder='passthrough',\n",
       "                     transformers=[('vec', TfidfVectorizer(), 'preprocess_data')])),\n",
       "  ('nb', MultinomialNB())],\n",
       " 'verbose': False,\n",
       " 'preprocess': ColumnTransformer(remainder='passthrough',\n",
       "                   transformers=[('vec', TfidfVectorizer(), 'preprocess_data')]),\n",
       " 'nb': MultinomialNB(),\n",
       " 'preprocess__n_jobs': None,\n",
       " 'preprocess__remainder': 'passthrough',\n",
       " 'preprocess__sparse_threshold': 0.3,\n",
       " 'preprocess__transformer_weights': None,\n",
       " 'preprocess__transformers': [('vec', TfidfVectorizer(), 'preprocess_data')],\n",
       " 'preprocess__verbose': False,\n",
       " 'preprocess__vec': TfidfVectorizer(),\n",
       " 'preprocess__vec__analyzer': 'word',\n",
       " 'preprocess__vec__binary': False,\n",
       " 'preprocess__vec__decode_error': 'strict',\n",
       " 'preprocess__vec__dtype': numpy.float64,\n",
       " 'preprocess__vec__encoding': 'utf-8',\n",
       " 'preprocess__vec__input': 'content',\n",
       " 'preprocess__vec__lowercase': True,\n",
       " 'preprocess__vec__max_df': 1.0,\n",
       " 'preprocess__vec__max_features': None,\n",
       " 'preprocess__vec__min_df': 1,\n",
       " 'preprocess__vec__ngram_range': (1, 1),\n",
       " 'preprocess__vec__norm': 'l2',\n",
       " 'preprocess__vec__preprocessor': None,\n",
       " 'preprocess__vec__smooth_idf': True,\n",
       " 'preprocess__vec__stop_words': None,\n",
       " 'preprocess__vec__strip_accents': None,\n",
       " 'preprocess__vec__sublinear_tf': False,\n",
       " 'preprocess__vec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'preprocess__vec__tokenizer': None,\n",
       " 'preprocess__vec__use_idf': True,\n",
       " 'preprocess__vec__vocabulary': None,\n",
       " 'nb__alpha': 1.0,\n",
       " 'nb__class_prior': None,\n",
       " 'nb__fit_prior': True}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c47a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocess',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('vec',\n",
       "                                                                         TfidfVectorizer(),\n",
       "                                                                         'preprocess_data')])),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'nb__alpha': (0.1, 1),\n",
       "                         'preprocess__vec__max_df': [0.9, 0.8],\n",
       "                         'preprocess__vec__min_df': [0.1, 0.05],\n",
       "                         'preprocess__vec__ngram_range': ((1, 1), (2, 2),\n",
       "                                                          (1, 2))},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# transform X features\n",
    "column_trans = ColumnTransformer(\n",
    "     [('vec', TfidfVectorizer(), 'preprocess_data')], remainder='passthrough')\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', column_trans),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters to search\n",
    "parameters = {\n",
    "    'preprocess__vec__ngram_range': ((1,1), (2,2), (1,2)),\n",
    "    'nb__alpha': (0.1,1),\n",
    "    'preprocess__vec__max_df': [1,0.9,0.8],\n",
    "    'preprocess__vec__min_df': [1,0.1,0.05]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a02c819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3044596223382885"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efbb6d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.1,\n",
       " 'preprocess__vec__max_df': 0.8,\n",
       " 'preprocess__vec__min_df': 0.05,\n",
       " 'preprocess__vec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f553d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
