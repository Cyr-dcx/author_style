{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36549d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "def csv_to_dataframes(output='ps'):\n",
    "    ''' Returns 2 dataframes\n",
    "\n",
    "    Extracts 1 dataframe with paragraphs and 1 dataframe with\n",
    "    sentences from a csv file. The csv files names' are parsed\n",
    "    assuming the following syntax:\n",
    "    \"author_name - title - publication_date.csv\"\n",
    "    '''\n",
    "    ##########################b#####################\n",
    "    ###y####  convert csv to df_paragraphs  ########\n",
    "    ################################################\n",
    "\n",
    "    # Get csv path ; the csv files are arrays of pre-selected* paragraphs\n",
    "    # that were extracted from raw txt files by * (cf. Lilou)\n",
    "    csv_path= \"/Users/cyrielle/code/Cyr-dcx/author_style/author_style/data/comp_aut\"\n",
    "\n",
    "\n",
    "    # Create a list of book names\n",
    "    books = [\n",
    "        csv_file for csv_file in os.listdir(csv_path)\n",
    "        if csv_file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Parsing csv file names to get author names, book titles and publishing date\n",
    "    # and putting these elements in lists that have the same index as the list 'books'\n",
    "    authors = [csv_file.split(' ')[0]+' '+csv_file.split(' ')[1] for csv_file in books]\n",
    "    titles = [csv_file.split(' - ')[1] for csv_file in books]\n",
    "    book_dates = [csv_file.split(' - ')[2].replace('.csv','') for csv_file in books]\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each book (in the list 'books'),\n",
    "    ## 1. create a dataframe with 1 paragraph per row\n",
    "    ## 2. create columns with fixed values for other features than text\n",
    "    ## 3. append the dataframe in the list 'dfs' of dataframes\n",
    "    ## containing the paragraphs from all books\n",
    "\n",
    "    for book in books:\n",
    "        ## 1.\n",
    "        df_temp = pd.read_csv(os.path.join(csv_path,book), header=None)\n",
    "        ## 2.\n",
    "        df_temp['author'] = authors[books.index(book)]\n",
    "        df_temp['title'] = titles[books.index(book)]\n",
    "        df_temp['book_date'] = book_dates[books.index(book)]\n",
    "        ## 3.\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    ## Concatenate all dataframes in 'dfs' to get\n",
    "    ## a single dataframe with paragraphs from all books\n",
    "    df_paragraphs = pd.concat([df for df in dfs], ignore_index = True, axis=0)\n",
    "    df_paragraphs.rename(mapper={0:\"text\"}, axis=1, inplace=True) # NB: The column name for the root_path text is explicitly called in a preprocessing function, it must be 'text'\n",
    "\n",
    "    ###############y########################################\n",
    "    ########  convert df_paragraphs to df_sentences  #######\n",
    "    #######################################b################\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each paragraph of our dataset (i.e. for each row in df_paragraph):\n",
    "    for i in range(df_paragraphs['text'].count()):\n",
    "\n",
    "        # Separate sentences with '. ' as a delimiter\n",
    "        # (careful: \"J. C.\", \"Mr.\", [...]) ignore ?\n",
    "        sentences = str(df_paragraphs.text[i]).split(\". \")\n",
    "\n",
    "        # Prepare columns with fixed values for Author_name, Title and Book_date,\n",
    "        # to assign each sentence of a paragraph to the same Author_name, Title and Book_date.\n",
    "        author_temp = [df_paragraphs.author[i] for k in range(len(sentences))]\n",
    "        title_temp = [df_paragraphs.title[i] for k in range(len(sentences))]\n",
    "        date_temp = [df_paragraphs.book_date[i] for k in range(len(sentences))]\n",
    "\n",
    "        # Concatenate the 4 previous lists to build a single dataframe\n",
    "        # containing all sentences of the i-th paragraph of df_paragraphs\n",
    "        data = [sentences, author_temp, title_temp, date_temp]\n",
    "        df_temp = pd.DataFrame(data).T\n",
    "\n",
    "        # Build the list of dataframes containing all sentences of our dataset\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    # Assemble the dataframe containing all sentences of our dataset\n",
    "    df_sentences = pd.concat(dfs, ignore_index = True, axis=0)\n",
    "    df_sentences.rename(mapper={0:\"text\", 1: 'author', 2:'title', 3 : 'book_date'}, axis=1, inplace=True)\n",
    "\n",
    "    if output == 'p':\n",
    "        return df_paragraphs\n",
    "    if output == 's':\n",
    "        return df_sentences\n",
    "    if output == 'ps':\n",
    "        return df_paragraphs, df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067e6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = csv_to_dataframes(output=\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668f14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import unidecode\n",
    "#import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "def preprocess(text,\n",
    "               punctuation=False,\n",
    "               lower_case=True,\n",
    "               remove_stopwords=False,\n",
    "               accents=True,\n",
    "               numbers=True,\n",
    "               lemmatize=False,\n",
    "               language='french'):\n",
    "\n",
    "    if numbers:\n",
    "        text = ''.join(char for char in text if not char.isdigit())\n",
    "    if punctuation:\n",
    "        text = ''.join(char for char in text if not char in string.punctuation)\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    if accents:\n",
    "        text = unidecode.unidecode(text)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(language))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = ' '.join(char for char in word_tokens if not char in stop_words)\n",
    "    if lemmatize:\n",
    "        text = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [lemmatizer.lemmatize(char) for char in text]\n",
    "        text = ' '.join(lemmatized)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_cleaned_column(df):\n",
    "    df[\"preprocess_data\"] = df['text'].apply(lambda x: preprocess(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"def return_token(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le texte de chaque token\n",
    "    return [X.text for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"def return_word_embedding(sentence):\n",
    "    # Vectoriser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le vecteur lié à chaque token\n",
    "    return [(X.vector) for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "def stopword_count(text):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopword_count = len([w for w in word_tokens if w in stop_words])\n",
    "    return stopword_count\n",
    "\n",
    "\n",
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    if total_length > 0:\n",
    "        return unique_word_length / total_length\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sentence_count(x):\n",
    "    if len(x.split()) >0:\n",
    "        return x.count('.') / len(x.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def features(df, output='p'):\n",
    "    if output=='p':\n",
    "\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['sentences_ratio'] = df['text'].apply(lambda x: 0 if len(x.split())==0 else x.count('.') / len(x.split()))\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n",
    "\n",
    "    elif output=='s':\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word_ratio'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92bcac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features(df, output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada763fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = csv_to_dataframes(output='p')\n",
    "df = features(df, output='p')\n",
    "\n",
    "#selection de X et y dans le dataframe df\n",
    "X = df[['preprocess_data','unique_word',\n",
    "        'word_ratio','sentences_ratio',\n",
    "        'stopwords_ratio','vocab richness']]\n",
    "y = df[\"author\"]\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_transformer = LabelEncoder()\n",
    "y = cat_transformer.fit_transform(y)\n",
    "\n",
    "\n",
    "#split date\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0793f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocess',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('vec',\n",
       "                                                                         TfidfVectorizer(),\n",
       "                                                                         'preprocess_data')])),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'nb__alpha': (0.1, 1),\n",
       "                         'preprocess__vec__max_df': [0.9, 0.8],\n",
       "                         'preprocess__vec__min_df': [0.1, 0.05],\n",
       "                         'preprocess__vec__ngram_range': ((1, 1), (2, 2),\n",
       "                                                          (1, 2))},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# transform X features\n",
    "column_trans = ColumnTransformer(\n",
    "     [('vec', TfidfVectorizer(), 'preprocess_data')], remainder='passthrough')\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', column_trans),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters to search\n",
    "parameters = {\n",
    "    'preprocess__vec__ngram_range': ((1,1), (2,2), (1,2)),\n",
    "    'nb__alpha': (0.1,1),\n",
    "    'preprocess__vec__max_df': [0.9,0.8],\n",
    "    'preprocess__vec__min_df': [0.1,0.05]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f69710c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32639614302932907"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77c13713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.1, 'preprocess__vec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97bb4ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocess',\n",
       "   ColumnTransformer(remainder='passthrough',\n",
       "                     transformers=[('vec', TfidfVectorizer(), 'preprocess_data')])),\n",
       "  ('nb', MultinomialNB())],\n",
       " 'verbose': False,\n",
       " 'preprocess': ColumnTransformer(remainder='passthrough',\n",
       "                   transformers=[('vec', TfidfVectorizer(), 'preprocess_data')]),\n",
       " 'nb': MultinomialNB(),\n",
       " 'preprocess__n_jobs': None,\n",
       " 'preprocess__remainder': 'passthrough',\n",
       " 'preprocess__sparse_threshold': 0.3,\n",
       " 'preprocess__transformer_weights': None,\n",
       " 'preprocess__transformers': [('vec', TfidfVectorizer(), 'preprocess_data')],\n",
       " 'preprocess__verbose': False,\n",
       " 'preprocess__vec': TfidfVectorizer(),\n",
       " 'preprocess__vec__analyzer': 'word',\n",
       " 'preprocess__vec__binary': False,\n",
       " 'preprocess__vec__decode_error': 'strict',\n",
       " 'preprocess__vec__dtype': numpy.float64,\n",
       " 'preprocess__vec__encoding': 'utf-8',\n",
       " 'preprocess__vec__input': 'content',\n",
       " 'preprocess__vec__lowercase': True,\n",
       " 'preprocess__vec__max_df': 1.0,\n",
       " 'preprocess__vec__max_features': None,\n",
       " 'preprocess__vec__min_df': 1,\n",
       " 'preprocess__vec__ngram_range': (1, 1),\n",
       " 'preprocess__vec__norm': 'l2',\n",
       " 'preprocess__vec__preprocessor': None,\n",
       " 'preprocess__vec__smooth_idf': True,\n",
       " 'preprocess__vec__stop_words': None,\n",
       " 'preprocess__vec__strip_accents': None,\n",
       " 'preprocess__vec__sublinear_tf': False,\n",
       " 'preprocess__vec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'preprocess__vec__tokenizer': None,\n",
       " 'preprocess__vec__use_idf': True,\n",
       " 'preprocess__vec__vocabulary': None,\n",
       " 'nb__alpha': 1.0,\n",
       " 'nb__class_prior': None,\n",
       " 'nb__fit_prior': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c91bcd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12445, 6), (12445,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e2c96b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocess_data</th>\n",
       "      <th>unique_word</th>\n",
       "      <th>word_ratio</th>\n",
       "      <th>sentences_ratio</th>\n",
       "      <th>stopwords_ratio</th>\n",
       "      <th>vocab richness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3694</th>\n",
       "      <td>- je me perds ? je prends mes aises. on me les...</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>87</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.657143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6195</th>\n",
       "      <td>tout est la dans quelques dizaines de metres c...</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>90</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.622642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10511</th>\n",
       "      <td>avec ses yeux cuits persilles de cils ras, son...</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>51</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.824561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16821</th>\n",
       "      <td>un coin semblait inaccessible. l'angle d'un de...</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>128</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.644578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11866</th>\n",
       "      <td>pourtant elle en voulait toujours vingt mille ...</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>174</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.539171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>a la michodiere, theatre mondain, ces evolutio...</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>42</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>il paraissait mal entendre. il s'etait mis a t...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>99</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.543307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>nous avons longe l'alameda ; sur le trottoir d...</td>\n",
       "      <td>0.809160</td>\n",
       "      <td>131</td>\n",
       "      <td>0.061069</td>\n",
       "      <td>0.320611</td>\n",
       "      <td>0.731034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>ty, confus, va botter les cotes des voltigeurs...</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>80</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.712871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>il s'assit sur le fauteuil qui etait recouvert...</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>32</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.837838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12445 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         preprocess_data  unique_word  \\\n",
       "3694   - je me perds ? je prends mes aises. on me les...     0.804598   \n",
       "6195   tout est la dans quelques dizaines de metres c...     0.722222   \n",
       "10511  avec ses yeux cuits persilles de cils ras, son...     0.901961   \n",
       "16821  un coin semblait inaccessible. l'angle d'un de...     0.804688   \n",
       "11866  pourtant elle en voulait toujours vingt mille ...     0.672414   \n",
       "...                                                  ...          ...   \n",
       "11284  a la michodiere, theatre mondain, ces evolutio...     0.880952   \n",
       "11964  il paraissait mal entendre. il s'etait mis a t...     0.666667   \n",
       "5390   nous avons longe l'alameda ; sur le trottoir d...     0.809160   \n",
       "860    ty, confus, va botter les cotes des voltigeurs...     0.825000   \n",
       "15795  il s'assit sur le fauteuil qui etait recouvert...     0.843750   \n",
       "\n",
       "       word_ratio  sentences_ratio  stopwords_ratio  vocab richness  \n",
       "3694           87         0.091954         0.425287        0.657143  \n",
       "6195           90         0.088889         0.377778        0.622642  \n",
       "10511          51         0.039216         0.352941        0.824561  \n",
       "16821         128         0.054688         0.453125        0.644578  \n",
       "11866         174         0.086207         0.419540        0.539171  \n",
       "...           ...              ...              ...             ...  \n",
       "11284          42         0.047619         0.357143        0.812500  \n",
       "11964          99         0.060606         0.525253        0.543307  \n",
       "5390          131         0.061069         0.320611        0.731034  \n",
       "860            80         0.075000         0.400000        0.712871  \n",
       "15795          32         0.062500         0.531250        0.837838  \n",
       "\n",
       "[12445 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57f52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
