{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13eace51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "def csv_to_dataframes(output='ps'):\n",
    "    ''' Returns 2 dataframes\n",
    "\n",
    "    Extracts 1 dataframe with paragraphs and 1 dataframe with\n",
    "    sentences from a csv file. The csv files names' are parsed\n",
    "    assuming the following syntax:\n",
    "    \"author_name - title - publication_date.csv\"\n",
    "    '''\n",
    "    ##########################b#####################\n",
    "    ###y####  convert csv to df_paragraphs  ########\n",
    "    ################################################\n",
    "\n",
    "    # Get csv path ; the csv files are arrays of pre-selected* paragraphs\n",
    "    # that were extracted from raw txt files by * (cf. Lilou)\n",
    "    csv_path= \"/Users/cyrielle/code/Cyr-dcx/author_style/author_style/data/comp_aut\"\n",
    "\n",
    "\n",
    "    # Create a list of book names\n",
    "    books = [\n",
    "        csv_file for csv_file in os.listdir(csv_path)\n",
    "        if csv_file.endswith('.csv')]\n",
    "\n",
    "\n",
    "    # Parsing csv file names to get author names, book titles and publishing date\n",
    "    # and putting these elements in lists that have the same index as the list 'books'\n",
    "    authors = [csv_file.split(' ')[0]+' '+csv_file.split(' ')[1] for csv_file in books]\n",
    "    titles = [csv_file.split(' - ')[1] for csv_file in books]\n",
    "    book_dates = [csv_file.split(' - ')[2].replace('.csv','') for csv_file in books]\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each book (in the list 'books'),\n",
    "    ## 1. create a dataframe with 1 paragraph per row\n",
    "    ## 2. create columns with fixed values for other features than text\n",
    "    ## 3. append the dataframe in the list 'dfs' of dataframes\n",
    "    ## containing the paragraphs from all books\n",
    "\n",
    "    for book in books:\n",
    "        ## 1.\n",
    "        df_temp = pd.read_csv(os.path.join(csv_path,book), header=None)\n",
    "        ## 2.\n",
    "        df_temp['author'] = authors[books.index(book)]\n",
    "        df_temp['title'] = titles[books.index(book)]\n",
    "        df_temp['book_date'] = book_dates[books.index(book)]\n",
    "        ## 3.\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    ## Concatenate all dataframes in 'dfs' to get\n",
    "    ## a single dataframe with paragraphs from all books\n",
    "    df_paragraphs = pd.concat([df for df in dfs], ignore_index = True, axis=0)\n",
    "    df_paragraphs.rename(mapper={0:\"text\"}, axis=1, inplace=True) # NB: The column name for the root_path text is explicitly called in a preprocessing function, it must be 'text'\n",
    "\n",
    "    ###############y########################################\n",
    "    ########  convert df_paragraphs to df_sentences  #######\n",
    "    #######################################b################\n",
    "\n",
    "    # Initializing a list of dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # For each paragraph of our dataset (i.e. for each row in df_paragraph):\n",
    "    for i in range(df_paragraphs['text'].count()):\n",
    "\n",
    "        # Separate sentences with '. ' as a delimiter\n",
    "        # (careful: \"J. C.\", \"Mr.\", [...]) ignore ?\n",
    "        sentences = str(df_paragraphs.text[i]).split(\". \")\n",
    "\n",
    "        # Prepare columns with fixed values for Author_name, Title and Book_date,\n",
    "        # to assign each sentence of a paragraph to the same Author_name, Title and Book_date.\n",
    "        author_temp = [df_paragraphs.author[i] for k in range(len(sentences))]\n",
    "        title_temp = [df_paragraphs.title[i] for k in range(len(sentences))]\n",
    "        date_temp = [df_paragraphs.book_date[i] for k in range(len(sentences))]\n",
    "\n",
    "        # Concatenate the 4 previous lists to build a single dataframe\n",
    "        # containing all sentences of the i-th paragraph of df_paragraphs\n",
    "        data = [sentences, author_temp, title_temp, date_temp]\n",
    "        df_temp = pd.DataFrame(data).T\n",
    "\n",
    "        # Build the list of dataframes containing all sentences of our dataset\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "    # Assemble the dataframe containing all sentences of our dataset\n",
    "    df_sentences = pd.concat(dfs, ignore_index = True, axis=0)\n",
    "    df_sentences.rename(mapper={0:\"text\", 1: 'author', 2:'title', 3 : 'book_date'}, axis=1, inplace=True)\n",
    "\n",
    "    if output == 'p':\n",
    "        return df_paragraphs\n",
    "    if output == 's':\n",
    "        return df_sentences\n",
    "    if output == 'ps':\n",
    "        return df_paragraphs, df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425bd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import unidecode\n",
    "#import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "def preprocess(text,\n",
    "               punctuation=False,\n",
    "               lower_case=True,\n",
    "               remove_stopwords=False,\n",
    "               accents=True,\n",
    "               numbers=True,\n",
    "               lemmatize=False,\n",
    "               language='french'):\n",
    "\n",
    "    if numbers:\n",
    "        text = ''.join(char for char in text if not char.isdigit())\n",
    "    if punctuation:\n",
    "        text = ''.join(char for char in text if not char in string.punctuation)\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    if accents:\n",
    "        text = unidecode.unidecode(text)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(language))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        text = ' '.join(char for char in word_tokens if not char in stop_words)\n",
    "    if lemmatize:\n",
    "        text = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [lemmatizer.lemmatize(char) for char in text]\n",
    "        text = ' '.join(lemmatized)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_cleaned_column(df):\n",
    "    df[\"preprocess_data\"] = df['text'].apply(lambda x: preprocess(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"def return_token(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le texte de chaque token\n",
    "    return [X.text for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"def return_word_embedding(sentence):\n",
    "    # Vectoriser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le vecteur lié à chaque token\n",
    "    return [(X.vector) for X in doc]\"\"\"\n",
    "\n",
    "\n",
    "def stopword_count(text):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopword_count = len([w for w in word_tokens if w in stop_words])\n",
    "    return stopword_count\n",
    "\n",
    "\n",
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    if total_length > 0:\n",
    "        return unique_word_length / total_length\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sentence_count(x):\n",
    "    if len(x.split()) >0:\n",
    "        return x.count('.') / len(x.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def features(df, output='p'):\n",
    "    if output=='p':\n",
    "\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['sentences_ratio'] = df['text'].apply(lambda x: 0 if len(x.split())==0 else x.count('.') / len(x.split()))\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n",
    "\n",
    "    elif output=='s':\n",
    "        df['preprocess_data'] = df['text'].apply(lambda x: preprocess(x))\n",
    "        df['word_ratio'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['unique_word_ratio'] = df['text'].apply(\n",
    "        lambda x: 0 if len(x.split())==0 else (len(np.unique(x.split()))/ len(x.split())))\n",
    "\n",
    "        df['stopwords_ratio'] = df['text'].apply(lambda x: 0 if len(x.split(\n",
    "    )) == 0 else (stopword_count(x) / len(x.split())))\n",
    "        df['vocab richness'] = df['text'].apply(vocab_richness)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad98a7",
   "metadata": {},
   "source": [
    "## Créer le dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d00fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = csv_to_dataframes(output=\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ee93b",
   "metadata": {},
   "source": [
    "## Appliquer le preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44041075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features(df, output='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eadae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fd7ed",
   "metadata": {},
   "source": [
    "## Créer X et y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7adebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selection de X et y dans le dataframe df\n",
    "X = df[['preprocess_data']]\n",
    "y = df[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094edb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocess_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pendant que nous franchissions la porte du nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;&lt; en  apres jesus-christ, les troupes de syag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>van eyck presenta la vierge au chanoine van de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>un an apres l'insolence du soldat, clovis rass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>les hommes se font une idee grotesque du temps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17774</th>\n",
       "      <td>c'est a bord d'un train de la southern pacific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17775</th>\n",
       "      <td>quelle que soit, pour signer, la solution adop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17776</th>\n",
       "      <td>reste la possibilite d'aller faire un tour dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17777</th>\n",
       "      <td>le lendemain matin, il se leve tard, trainant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17778</th>\n",
       "      <td>il part en direction de la gare maritime du ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17779 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         preprocess_data\n",
       "0      pendant que nous franchissions la porte du nor...\n",
       "1      << en  apres jesus-christ, les troupes de syag...\n",
       "2      van eyck presenta la vierge au chanoine van de...\n",
       "3      un an apres l'insolence du soldat, clovis rass...\n",
       "4      les hommes se font une idee grotesque du temps...\n",
       "...                                                  ...\n",
       "17774  c'est a bord d'un train de la southern pacific...\n",
       "17775  quelle que soit, pour signer, la solution adop...\n",
       "17776  reste la possibilite d'aller faire un tour dan...\n",
       "17777  le lendemain matin, il se leve tard, trainant ...\n",
       "17778  il part en direction de la gare maritime du ha...\n",
       "\n",
       "[17779 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9641664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.preprocessing import LabelEncoder'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Label Encode y\n",
    "cat_transformer = LabelEncoder()\n",
    "y = cat_transformer.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bde7d",
   "metadata": {},
   "source": [
    "## Sauvegarder X et y sur le package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87e48db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t8/clhcfp2n5sgcs4b81syf8xym0000gp/T/ipykernel_26992/1187991563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"X.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"y.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(X,\"X.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39f55c04",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t8/clhcfp2n5sgcs4b81syf8xym0000gp/T/ipykernel_26992/4242886273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"y.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "joblib.dump(y,\"y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed5fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
